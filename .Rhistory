head(Datos)
#Calculamos la cantidad de palabras para crear el hash table.
n<-length(Datos$palabra)
#Creamos un hash table endogeno, para tener todo el dataframe numerico y asi poder aplicar PCA.
#Es decir cada palabra representa un numero. Ej: 1->votar, 2->chiabe, etc.
IDpalabra = array(1:n)
#Creamos un nuevo dataframe con todos los valores numericos.
newDatos = data.frame(IDpalabra,Datos$repetida)
colnames(newDatos)[2] <- "repetida"
#Colocamos nombres a las filas
rownames(newDatos) <- Datos[,1]
plot(newDatos, pch = 19)
grupos <- kmeans(newDatos, , iter.max = 100)  ## iter.max por defecto es 10
plot(newDatos, pch = 19)
points(grupos$centers, pch = 19, col = "yellow", cex = 3)
points(newDatos, col = grupos$cluster + 1, pch = 19)
grupos <- kmeans(newDatos,3 , iter.max = 100)  ## iter.max por defecto es 10
#Podemos observar los centroides y los distintos grupos
plot(newDatos, pch = 19)
points(grupos$centers, pch = 19, col = "yellow", cex = 3)
points(newDatos, col = grupos$cluster + 1, pch = 19)
grupos <- kmeans(newDatos,4 , iter.max = 100)  ## iter.max por defecto es 10
#Podemos observar los centroides y los distintos grupos
plot(newDatos, pch = 19)
points(grupos$centers, pch = 19, col = "yellow", cex = 3)
points(newDatos, col = grupos$cluster + 1, pch = 19)
grupos <- kmeans(newDatos,3, iter.max = 100)  ## El 2do parametro es el IMPORTANTE
grupos
grupos$cluster
grupos$centers
grupos$totss  # Inercia Total
grupos$withinss  # Inercia Intra-clases por grupo (una para cada grupo)
grupos$tot.withinss  # Inercia Intra-clases
grupos$betweenss  # inercia Inter-clases
# Verificación del Teorema de Fisher
grupos$totss == grupos$tot.withinss + grupos$betweenss
grupos$size  # Tamaño de las clases
#Podemos observar los centroides y los distintos grupos
plot(newDatos, pch = 19)
points(grupos$centers, pch = 19, col = "yellow", cex = 3)
points(newDatos, col = grupos$cluster + 1, pch = 19)
rownames(grupos$centers) <- c("Cluster 1", "Cluster 2", "Cluster 3")
barplot(t(grupos$centers), beside = TRUE, col = heat.colors(5))
NDatos <- cbind(newDatos, Grupo = grupos$cluster)
head(NDatos)
grupos <- kmeans(newDatos, 4, iter.max = 50)
plot(newDatos, pch = 19, xlab = expression(x[1]), ylab = expression(x[2]))
points(grupos$centers, pch = 19, col = "blue", cex = 2)
points(newDatos, col = grupos$cluster + 1, pch = 19)
grupos <- kmeans(newDatos, 4, iter.max = 50)
plot(newDatos, pch = 19, xlab = expression(x[1]), ylab = expression(x[2]))
points(grupos$centers, pch = 19, col = "blue", cex = 2)
points(newDatos, col = grupos$cluster + 1, pch = 19)
lot(newDatos, pch = 19)
plot(newDatos, pch = 19)
InerciaIC = rep(0, 30)
for (k in 1:30) {
grupos = kmeans(newDatos, k)
InerciaIC[k] = grupos$tot.withinss
}
plot(InerciaIC, col = "blue", type = "b")
modelo = hclust(dist(newDatos), method = "complete")
plot(modelo)
rect.hclust(modelo, k = 4, border = "red")
Grupo <- cutree(modelo, k = 3)
NDatos <- cbind(newDatos, Grupo)
NDatos
centros <- centers.hclust(newDatos, modelo, nclust = 3, use.median = FALSE)
centros
modelo = hclust(dist(newDatos), method = "average")
plot(modelo)
modelo = hclust(dist(newDatos), method = "single")
plot(modelo)
modelo = hclust(dist(newDatos), method = "ward")
modelo = hclust(dist(newDatos), method = "ward.D")
plot(modelo)
CJA <- subset(newDatos, newDatos$repetida > 400)
CJA
modelo = hclust(dist(CJA), method = "complete")
plot(modelo)
modelo = hclust(dist(CJA), method = "average")
plot(modelo)
modelo = hclust(dist(CJA), method = "single")
plot(modelo)
modelo = hclust(dist(CJA), method = "ward.D")
plot(modelo)
modelo = hclust(dist(CJA), method = "single")
plot(modelo)
rect.hclust(modelo, k = 4, border = "red")
rect.hclust(modelo, k = 3, border = "red")
Grupo <- cutree(modelo, k = 3)
NDatos <- cbind(CJA, Grupo)
NDatos
centros <- centers.hclust(CJA, modelo, nclust = 3, use.median = FALSE)
centros
View(CJA)
CJA <- subset(newDatos, newDatos$repetida > 200)
modelo = hclust(dist(CJA), method = "single")
#Resaltamos en rojo los grupos.
plot(modelo)
rect.hclust(modelo, k = 3, border = "red")
#Le asignamos un gripo a cada palabra.
Grupo <- cutree(modelo, k = 3)
NDatos <- cbind(CJA, Grupo)
NDatos
#Mostramos los centros de cada clasificacion
centros <- centers.hclust(CJA, modelo, nclust = 3, use.median = FALSE)
centros
CJA <- subset(newDatos, newDatos$repetida > 300)
#Usando la agregación del salto máximo
#modelo = hclust(dist(CJA), method = "complete")
#Usando la agregación por promedios
#modelo = hclust(dist(CJA), method = "average")
#Usando la agregación de Ward
#modelo = hclust(dist(CJA), method = "ward.D")
#Usando la agregación del salto mínimo
modelo = hclust(dist(CJA), method = "single")
#Resaltamos en rojo los grupos.
plot(modelo)
rect.hclust(modelo, k = 3, border = "red")
#Le asignamos un gripo a cada palabra.
Grupo <- cutree(modelo, k = 3)
NDatos <- cbind(CJA, Grupo)
NDatos
#Mostramos los centros de cada clasificacion, visualmente no son los centros.
centros <- centers.hclust(CJA, modelo, nclust = 3, use.median = FALSE)
centros
rownames(centros) <- c("Cluster 1", "Cluster 2", "Cluster 3")
barplot(centros[1, ], col = c(2, 3, 4, 5, 6, 7), las = 2)
barplot(centros[2, ], col = c(2, 3, 4, 5, 6, 7), las = 2)
barplot(centros[3, ], col = c(2, 3, 4, 5, 6, 7), las = 2)
barplot(t(centros), beside = TRUE, col = c(2, 3, 4, 5, 6, 7))
#Gráfica de los centro para el cluster 1
rownames(centros) <- c("Cluster 1", "Cluster 2", "Cluster 3")
barplot(centros[1, ], col = c(2, 3, 4, 5, 6, 7), las = 2)
barplot(t(centros), beside = TRUE, col = c(2, 3, 4, 5, 6, 7))
CJA <- subset(newDatos, newDatos$repetida > 300)
distMatrix <- dist(scale(CJA))
modelo <- hclust(distMatrix, method = "ward.D")
plot(modelo)
rect.hclust(modelo, k = 3)
modelo <- hclust(distMatrix, method = "single")
plot(modelo)
rect.hclust(modelo, k = 3)
CJA2 <- subset(newDatos, newDatos$repetida > 1 &  newDatos$repetida < 10 )
CJA2
View(CJA2)
CJA1 <- subset(newDatos, newDatos$repetida > 100 &  newDatos$repetida < 150 )
View(CJA1)
View(CJA2)
CJA2 <- subset(newDatos, newDatos$repetida > 100 &  newDatos$repetida < 120 )
View(CJA1)
CJA2 <- subset(newDatos, newDatos$repetida > 100 &  newDatos$repetida < 120 )
View(CJA2)
CJA<-CJA+CJA1+CJA2
CJA<- data.frame(CJA, CJA1,CJA2)
CJA<- rbind(CJA, CJA1,CJA2)
CJA
View(CJA)
modelo = hclust(dist(CJA), method = "single")
#Resaltamos en rojo los grupos.
plot(modelo)
rect.hclust(modelo, k = 3, border = "red")
modelo = prcomp(newDatos)
biplot(modelo)
suppressMessages(library(FactoMineR))
modelo <- PCA(newDatos, scale.unit = TRUE, ncp = 5, graph = FALSE)
modelo$eig
par(mfrow = c(1, 2)) # dividimos la pantalla para recibir dos gráficos.
plot(modelo, axes = c(1, 2), choix = "ind", col.ind = "red", new.plot = TRUE)
plot(modelo, axes = c(1, 2), choix = "var", col.var = "blue", new.plot = TRUE)
cos2.ind <- (modelo$ind$cos2[, 1] + modelo$ind$cos2[, 2]) * 100
cos2.ind
idx <- which(dimnames(dtm)$Terms == "r")
inspect(dtm[idx + (0:5), 101:110])
idx <- which(dimnames(dtm)$Terms == "c")
inspect(dtm[idx + (0:5), 101:110])
dtm <- DocumentTermMatrix(corpus, control = list(wordLengths = c(1, Inf)))
idx <- which(dimnames(dtm)$Terms == "v")
inspect(dtm[idx + (0:5), 101:110])
findAssocs(dtm, "r", 0.2)
library(topicmodels)
install.packages("topicmodels")
dtm <- as.DocumentTermMatrix(dtm)
lda <- LDA(dtm, k = 8) # find 8 topics
library('ProjectTemplate')
library('tm')
library(wordcloud)
library(ggplot2)
library(Rstem)
library(SnowballC)
library(topicmodels)
#CAMBIAR DIRECCION
setwd("C:/Users/EricBellet/Documents/Asignacion1/Asignacion1")
load.project()
#Data Import
load(file = "C:/Users/EricBellet/Desktop/Asignacion1/data/tw.Rdata" )
#Cargamos los tweets
tweets <- tw$text
#Debido a que el dataset tiene los signos de puntuacion en un formato distinto, hay que transformarlos para poder manejarlos.
tweets = iconv(tweets, to="ASCII//TRANSLIT")
#str(tweets) = 'data.frame':	6750 obs. of  17 variables
#Separamos por espacio.
tweets_text <- paste(tweets, collapse=" ")
#Ahora creamos un corpus.
#tweets_source <- VectorSource(tweets_text)
#corpus <- Corpus(tweets_source)
corpus <- Corpus(VectorSource(tweets_text), readerControl = list(language = "es"))
#-------------------------------------------
#Comenzamos con la limpieza de los textos.
#Elimina los signos de puntuacion.
corpus <- tm_map(corpus, removePunctuation)
#Elimina los espacios.
corpus <- tm_map(corpus, stripWhitespace)
#Convierte todo en minusculas.
corpus <- tm_map(corpus, content_transformer(tolower))
#Elimina palabras vacías (stopwords) en español
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
#Elimina los URL.
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
removeURL <- function(x) gsub("https[^[:space:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
#Hay muchos tweets que tienen htt..., lo cual no significa nada y no aporta nada para nuestro analisis.
corpus <- tm_map(corpus, content_transformer(removeWords), c("htt...","ht...", "htt. ","ht.","6d","rt","retweet","q","d","i"))
#Segun estas funciones hacen lo mismo, lematizan. Pero lo hacen mal, leyendo blogs dicen que no hay nada que en R que lematice bien.
#corpus <- tm_map(corpus, stemDocument)
#corpus <- wordStem(corpus, "spanish")
#-------------------------------------------
#Creamos el document-term matrix.
dtm <- DocumentTermMatrix(corpus, control = list(wordLengths = c(1, Inf)))
idx <- which(dimnames(dtm)$Terms == "r")
findAssocs(dtm, "r", 0.2)
inspect(dtm[idx + (0:5), 101:110])
#Lo convertimos en una matriz
dtm2 <- as.matrix(dtm)
#Calculamos las mas frecuentes.
frequency <- colSums(dtm2)
#Ordenamos el vector por las palabras mas frecuentes.
frequency <- sort(frequency, decreasing=TRUE)
#head(frequency)
#-------------------------------------------
term.freq <- frequency
#Vamos a realizar el analisis con las palabras que esten repetidas mas de ...
term.freq <- subset(term.freq, term.freq >=1) #ESTO ES IMPORTANTE.
#Observar las palabras mas frecuentes (100 o mas)
#(frequency <- findFreqTerms(dtm, lowfreq=100))
df <- data.frame(term = names(term.freq), freq = term.freq)
data <- data.frame(df$term, df$freq)
colnames(data)[1] <- "palabra"
colnames(data)[2] <- "repetida"
#Mostramos un grafico de las palabras y cuantas veces estan repetidas.
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
dtm <- as.DocumentTermMatrix(dtm)
lda <- LDA(dtm, k = 8) # find 8 topics
(term <- terms(lda, 6)) # first 6 terms of every topic
# first topic identified for every document (tweet)
topic <- topics(lda, 1)
topics <- data.frame(date=as.IDate(tweets.df$created), topic)
qplot(date, ..count.., data=topics, geom="density",
fill=term[topic], position="stack")
#-------------------------------------------
save (data,file="data/preprocess.RData")
topic <- topics(lda, 1)
topics <- data.frame(date=as.IDate(data$repetida), topic)
qplot(date, ..count.., data=topics, geom="density",
fill=term[topic], position="stack")
(term <- terms(lda, 6)) # first 6 terms of every topic
topic <- topics(lda, 1)
topics <- data.frame(date=data$repetida, topic)
topic
topic <- topics(lda, 1)
topics <- data.frame(date=as.IDate(tweets.data$palabra), topic)
topics <- data.frame(date=(tweets.data$palabra), topic)
library('ProjectTemplate')
library('tm')
library(wordcloud)
library(ggplot2)
#CAMBIAR DIRECCION
setwd("C:/Users/EricBellet/Documents/Asignacion1/Asignacion1")
load.project()
#Data Import
load(file = "C:/Users/EricBellet/Desktop/Asignacion1/data/tw.Rdata" )
#Cargamos los tweets
tweets <- tw$text
#Debido a que el dataset tiene los signos de puntuacion en un formato distinto, hay que transformarlos para poder manejarlos.
tweets = iconv(tweets, to="ASCII//TRANSLIT")
#str(tweets) = 'data.frame':	6750 obs. of  17 variables
#Separamos por espacio.
tweets_text <- paste(tweets, collapse=" ")
#Ahora creamos un corpus.
#tweets_source <- VectorSource(tweets_text)
#corpus <- Corpus(tweets_source)
corpus <- Corpus(VectorSource(tweets_text), readerControl = list(language = "es"))
#-------------------------------------------
#Comenzamos con la limpieza de los textos.
#Elimina los signos de puntuacion.
corpus <- tm_map(corpus, removePunctuation)
#Elimina los espacios.
corpus <- tm_map(corpus, stripWhitespace)
#Convierte todo en minusculas.
corpus <- tm_map(corpus, content_transformer(tolower))
#Elimina palabras vacías (stopwords) en español
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
#Elimina los URL.
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
removeURL <- function(x) gsub("https[^[:space:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
#Hay muchos tweets que tienen htt..., lo cual no significa nada y no aporta nada para nuestro analisis.
corpus <- tm_map(corpus, content_transformer(removeWords), c("htt...","ht...", "htt. ","ht.","6d","rt","retweet","q","d","i"))
#Segun estas funciones hacen lo mismo, lematizan. Pero lo hacen mal, leyendo blogs dicen que no hay nada que en R que lematice bien.
#corpus <- tm_map(corpus, stemDocument)
#corpus <- wordStem(corpus, "spanish")
#-------------------------------------------
#Creamos el document-term matrix.
dtm <- DocumentTermMatrix(corpus, control = list(wordLengths = c(1, Inf)))
idx <- which(dimnames(dtm)$Terms == "r")
findAssocs(dtm, "r", 0.2)
inspect(dtm[idx + (0:5), 101:110])
#Lo convertimos en una matriz
dtm2 <- as.matrix(dtm)
#Calculamos las mas frecuentes.
frequency <- colSums(dtm2)
#Ordenamos el vector por las palabras mas frecuentes.
frequency <- sort(frequency, decreasing=TRUE)
#head(frequency)
#-------------------------------------------
term.freq <- frequency
#Vamos a realizar el analisis con las palabras que esten repetidas mas de ...
term.freq <- subset(term.freq, term.freq >=1) #ESTO ES IMPORTANTE.
#Observar las palabras mas frecuentes (100 o mas)
#(frequency <- findFreqTerms(dtm, lowfreq=100))
df <- data.frame(term = names(term.freq), freq = term.freq)
data <- data.frame(df$term, df$freq)
colnames(data)[1] <- "palabra"
colnames(data)[2] <- "repetida"
#Mostramos un grafico de las palabras y cuantas veces estan repetidas.
df2 <- subset(data, data$repetida > 400)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
#-------------------------------------------
save (data,file="data/preprocess.RData")
View(df2)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
df2 <- subset(data, data$repetida > 400)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
ggplot(df2, aes(x=data$palabra, y=data$repetida)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
term.freq <- subset(term.freq, term.freq >=400)
df2 <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
term.freq <- subset(term.freq, term.freq >=300)
df2 <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
#-------------------------------------------
term.freq <- subset(term.freq, term.freq >=100)
df2 <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
term.freq <- subset(term.freq, term.freq >=50)
df2 <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
library('ProjectTemplate')
library('tm')
library(wordcloud)
library(ggplot2)
#CAMBIAR DIRECCION
setwd("C:/Users/EricBellet/Documents/Asignacion1/Asignacion1")
load.project()
#Data Import
load(file = "C:/Users/EricBellet/Desktop/Asignacion1/data/tw.Rdata" )
#Cargamos los tweets
tweets <- tw$text
#Debido a que el dataset tiene los signos de puntuacion en un formato distinto, hay que transformarlos para poder manejarlos.
tweets = iconv(tweets, to="ASCII//TRANSLIT")
#str(tweets) = 'data.frame':	6750 obs. of  17 variables
#Separamos por espacio.
tweets_text <- paste(tweets, collapse=" ")
#Ahora creamos un corpus.
#tweets_source <- VectorSource(tweets_text)
#corpus <- Corpus(tweets_source)
corpus <- Corpus(VectorSource(tweets_text), readerControl = list(language = "es"))
#-------------------------------------------
#Comenzamos con la limpieza de los textos.
#Elimina los signos de puntuacion.
corpus <- tm_map(corpus, removePunctuation)
#Elimina los espacios.
corpus <- tm_map(corpus, stripWhitespace)
#Convierte todo en minusculas.
corpus <- tm_map(corpus, content_transformer(tolower))
#Elimina palabras vacías (stopwords) en español
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
#Elimina los URL.
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
removeURL <- function(x) gsub("https[^[:space:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
#Hay muchos tweets que tienen htt..., lo cual no significa nada y no aporta nada para nuestro analisis.
corpus <- tm_map(corpus, content_transformer(removeWords), c("htt...","ht...", "htt. ","ht.","6d","rt","retweet","q","d","i"))
#Segun estas funciones hacen lo mismo, lematizan. Pero lo hacen mal, leyendo blogs dicen que no hay nada que en R que lematice bien.
#corpus <- tm_map(corpus, stemDocument)
#corpus <- wordStem(corpus, "spanish")
#-------------------------------------------
#Creamos el document-term matrix.
dtm <- DocumentTermMatrix(corpus, control = list(wordLengths = c(1, Inf)))
idx <- which(dimnames(dtm)$Terms == "r")
findAssocs(dtm, "r", 0.2)
inspect(dtm[idx + (0:5), 101:110])
#Lo convertimos en una matriz
dtm2 <- as.matrix(dtm)
#Calculamos las mas frecuentes.
frequency <- colSums(dtm2)
#Ordenamos el vector por las palabras mas frecuentes.
frequency <- sort(frequency, decreasing=TRUE)
#head(frequency)
#-------------------------------------------
term.freq <- frequency
#Vamos a realizar el analisis con las palabras que esten repetidas mas de ...
term.freq <- subset(term.freq, term.freq >=1) #ESTO ES IMPORTANTE.
#Observar las palabras mas frecuentes (100 o mas)
#(frequency <- findFreqTerms(dtm, lowfreq=100))
df <- data.frame(term = names(term.freq), freq = term.freq)
data <- data.frame(df$term, df$freq)
colnames(data)[1] <- "palabra"
colnames(data)[2] <- "repetida"
#Mostramos un grafico de las palabras y cuantas veces estan repetidas.
term.freq <- subset(term.freq, term.freq >=50)
df2 <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
#-------------------------------------------
save (data,file="data/preprocess.RData")
term.freq <- subset(term.freq, term.freq >=100)
df2 <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
term.freq <- subset(term.freq, term.freq >=400)
df2 <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
term.freq <- subset(term.freq, term.freq >=250)
df2 <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
term.freq2 <- subset(term.freq, term.freq >=250)
library('ProjectTemplate')
library('tm')
library(wordcloud)
library(ggplot2)
#CAMBIAR DIRECCION
setwd("C:/Users/EricBellet/Documents/Asignacion1/Asignacion1")
load.project()
#Data Import
load(file = "C:/Users/EricBellet/Desktop/Asignacion1/data/tw.Rdata" )
#Cargamos los tweets
tweets <- tw$text
#Debido a que el dataset tiene los signos de puntuacion en un formato distinto, hay que transformarlos para poder manejarlos.
tweets = iconv(tweets, to="ASCII//TRANSLIT")
#str(tweets) = 'data.frame':	6750 obs. of  17 variables
#Separamos por espacio.
tweets_text <- paste(tweets, collapse=" ")
#Ahora creamos un corpus.
#tweets_source <- VectorSource(tweets_text)
#corpus <- Corpus(tweets_source)
corpus <- Corpus(VectorSource(tweets_text), readerControl = list(language = "es"))
#-------------------------------------------
#Comenzamos con la limpieza de los textos.
#Elimina los signos de puntuacion.
corpus <- tm_map(corpus, removePunctuation)
#Elimina los espacios.
corpus <- tm_map(corpus, stripWhitespace)
#Convierte todo en minusculas.
corpus <- tm_map(corpus, content_transformer(tolower))
#Elimina palabras vacías (stopwords) en español
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
#Elimina los URL.
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
removeURL <- function(x) gsub("https[^[:space:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
#Hay muchos tweets que tienen htt..., lo cual no significa nada y no aporta nada para nuestro analisis.
corpus <- tm_map(corpus, content_transformer(removeWords), c("htt...","ht...", "htt. ","ht.","6d","rt","retweet","q","d","i"))
#Segun estas funciones hacen lo mismo, lematizan. Pero lo hacen mal, leyendo blogs dicen que no hay nada que en R que lematice bien.
#corpus <- tm_map(corpus, stemDocument)
#corpus <- wordStem(corpus, "spanish")
#-------------------------------------------
#Creamos el document-term matrix.
dtm <- DocumentTermMatrix(corpus, control = list(wordLengths = c(1, Inf)))
idx <- which(dimnames(dtm)$Terms == "r")
findAssocs(dtm, "r", 0.2)
inspect(dtm[idx + (0:5), 101:110])
#Lo convertimos en una matriz
dtm2 <- as.matrix(dtm)
#Calculamos las mas frecuentes.
frequency <- colSums(dtm2)
#Ordenamos el vector por las palabras mas frecuentes.
frequency <- sort(frequency, decreasing=TRUE)
#head(frequency)
#-------------------------------------------
term.freq <- frequency
#Vamos a realizar el analisis con las palabras que esten repetidas mas de ...
term.freq <- subset(term.freq, term.freq >=1) #ESTO ES IMPORTANTE.
#Observar las palabras mas frecuentes (100 o mas)
#(frequency <- findFreqTerms(dtm, lowfreq=100))
df <- data.frame(term = names(term.freq), freq = term.freq)
data <- data.frame(df$term, df$freq)
colnames(data)[1] <- "palabra"
colnames(data)[2] <- "repetida"
#Mostramos un grafico de algunas de las palabras y cuantas veces estan repetidas.
term.freq2 <- subset(term.freq, term.freq >=250)
df2 <- data.frame(term = names(term.freq2), freq = term.freq2)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
#-------------------------------------------
save (data,file="data/preprocess.RData")
term.freq2 <- subset(term.freq, term.freq >=150)
df2 <- data.frame(term = names(term.freq2), freq = term.freq2)
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") +coord_flip()
